{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "pip install transformers"
      ],
      "metadata": {
        "id": "4IU8ZBPJ6CK5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qRdXCXGf5xeI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT MLM"
      ],
      "metadata": {
        "id": "kTPB8eoFuL63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased\")\n",
        "bert.eval()\n",
        "tokenizer  = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "z3_N2I9P59io"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.special_tokens_map"
      ],
      "metadata": {
        "id": "vGIungJZ6VPQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6cf50fc-4dc5-41a1-a2b4-b3cfb490d53e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'unk_token': '[UNK]',\n",
              " 'sep_token': '[SEP]',\n",
              " 'pad_token': '[PAD]',\n",
              " 'cls_token': '[CLS]',\n",
              " 'mask_token': '[MASK]'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_text = \"paris is the [MASK] of france.\"\n",
        "tokenized_masked_text = tokenizer(masked_text, return_tensors=\"pt\")[\"input_ids\"]\n",
        "print(\"Text input:\")\n",
        "print(tokenizer.decode(tokenized_masked_text[0], skip_special_tokens=False))"
      ],
      "metadata": {
        "id": "MurMKlX16Qev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30404e3a-0e33-438e-8cfa-7d136fdd9fe0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text input:\n",
            "[CLS] paris is the [MASK] of france. [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.mask_token_id"
      ],
      "metadata": {
        "id": "Es9YwdxIIZ3R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d67775c-b599-496b-9fc3-17596e50b23f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "103"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_masked_text"
      ],
      "metadata": {
        "id": "nUcPh8lbHozt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d31e0483-c504-4a14-db71-2690c161fd33"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 101, 3000, 2003, 1996,  103, 1997, 2605, 1012,  102]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = bert(tokenized_masked_text)\n",
        "logits = output.logits\n"
      ],
      "metadata": {
        "id": "VwiwoqXyHan6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape"
      ],
      "metadata": {
        "id": "0G6pSGhnIj4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51483f39-8f11-4885-86ac-e81cb44afb9c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 9, 30522])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7oUJQARQIj-n"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_argtopk(logits, tokenizer, id, k=1):\n",
        "    topk = torch.topk(logits, axis=-1, k=k, sorted=True)\n",
        "    indices = topk.indices\n",
        "    values = topk.values\n",
        "\n",
        "    for k in range(k):\n",
        "        print(tokenizer.decode(indices[0, id:id+1, k]), np.round(values[0, id, k].item(), 3))"
      ],
      "metadata": {
        "id": "pp3ZxZeoIw_1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_argtopk(logits.detach(), tokenizer, 4, k=5)"
      ],
      "metadata": {
        "id": "iWMaIf_9Ilia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d409150-29e3-4891-9edb-88844ec16817"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "capital 17.351\n",
            "birthplace 11.668\n",
            "northernmost 10.505\n",
            "centre 10.466\n",
            "southernmost 10.214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Next sentence prediction"
      ],
      "metadata": {
        "id": "Z4ijCqwuuXjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForNextSentencePrediction"
      ],
      "metadata": {
        "id": "k7YLt90Du7Ak"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert = BertForNextSentencePrediction.from_pretrained(\"bert-base-uncased\")\n",
        "bert.eval();\n"
      ],
      "metadata": {
        "id": "vzpgxx3HuauH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s1 = \"paris is the capital of france.\"\n",
        "s2 = \"the malayan tiger is native to peninsular malaysia.\"\n",
        "\n",
        "encoded_sentences = tokenizer(s1, s2, return_token_type_ids=True, return_tensors=\"pt\")\n",
        "print(encoded_sentences)"
      ],
      "metadata": {
        "id": "bOr7OMdSvC0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aebc9304-278c-4698-f5e7-b4559d1fa62f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  3000,  2003,  1996,  3007,  1997,  2605,  1012,   102,  1996,\n",
            "         19979,  2078,  6816,  2003,  3128,  2000, 22682,  6027,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = bert(**encoded_sentences)"
      ],
      "metadata": {
        "id": "Wr6-37C2vB32"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.logits"
      ],
      "metadata": {
        "id": "mUHLD18vvbCI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b05be4c-76bb-4af2-d2de-2ab35a484d3a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-3.4455,  6.6671]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s1 = \"the dog is barking.\"\n",
        "s2 = \"he saw a cat.\"\n",
        "\n",
        "encoded_sentences = tokenizer(s1, s2, return_token_type_ids=True, return_tensors=\"pt\")\n",
        "out = bert(**encoded_sentences)\n",
        "out.logits\n"
      ],
      "metadata": {
        "id": "qwarz_POwHUW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d28a34c5-ba39-483c-a277-3e1502c092e1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3.8689, -2.8081]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2"
      ],
      "metadata": {
        "id": "bmV2RvKOqqC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from tabulate import tabulate"
      ],
      "metadata": {
        "id": "BPdhezfuqs7T"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
        "gpt.eval();"
      ],
      "metadata": {
        "id": "nZwR_QJVrRi2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")"
      ],
      "metadata": {
        "id": "AsjEXHp1q7pm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Paris is the capital of\"\n",
        "tokenized_text = tokenizer.encode(text)\n",
        "print(tokenizer.decode(tokenized_text, add_special_tokens=True))\n",
        "print(tokenizer.tokenize(text))"
      ],
      "metadata": {
        "id": "xqOoyUwlrd6G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "047de23e-6272-4d23-d2c1-4fa04fc8765b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paris is the capital of\n",
            "['Paris', 'Ġis', 'Ġthe', 'Ġcapital', 'Ġof']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Paris is the capital of\"\n",
        "tokenized_text = tokenizer.encode(text)\n",
        "input_tensor = torch.tensor([tokenizer.bos_token_id] + tokenized_text)[None, :]\n",
        "with torch.no_grad():\n",
        "    out = gpt(input_tensor)\n",
        "logits = out.logits.detach().numpy()\n",
        "logits_sorted = np.argsort(logits[0], axis=-1)\n",
        "top_5_logits = logits_sorted[:, -5:][:, ::-1]\n"
      ],
      "metadata": {
        "id": "uNvZMdr3rsQO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape"
      ],
      "metadata": {
        "id": "RwxU7DNYaAEd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e9890ab-c927-4495-a5f4-1730596b1772"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 6, 50257)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_5_logits.shape"
      ],
      "metadata": {
        "id": "a_RqlpaVxCkY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee8b392-0c80-44cc-a8de-f0b8e1241b40"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(top_5_logits)"
      ],
      "metadata": {
        "id": "D7QIt1cMuLyf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2267f6c-b5d0-44ff-83c7-795b5b94d9da"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['TheAThisInIt',\n",
              " ' (, Saint:-',\n",
              " ' the a set to home',\n",
              " ' capital most world city latest',\n",
              " ' of city and, for',\n",
              " ' France the Europe French Paris']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_tokens = []\n",
        "input_tokens = [tokenizer.bos_token] + tokenizer.tokenize(text)\n",
        "for i in range(top_5_logits.shape[0]):\n",
        "    tokens = [input_tokens[i]] + [tokenizer.decode(tok) for tok in top_5_logits[i]]\n",
        "    tokens = [t.replace(\"Ġ\", \" \") for t in tokens]\n",
        "    list_tokens.append(tokens)\n"
      ],
      "metadata": {
        "id": "xbZRnkjHsVl4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = np.array(list_tokens)\n",
        "print(\"Encoded inputs:\")\n",
        "print(tabulate(table, headers=[\"Input tokens\"] + [f\"Top {i}\" for i in range(1, 6)], tablefmt=\"fancy_grid\"))"
      ],
      "metadata": {
        "id": "2yl-qcBqsAA3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b2a2b6c-593c-42bf-c3c2-a311e72ad516"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded inputs:\n",
            "╒════════════════╤═════════╤═════════╤═════════╤═════════╤═════════╕\n",
            "│ Input tokens   │ Top 1   │ Top 2   │ Top 3   │ Top 4   │ Top 5   │\n",
            "╞════════════════╪═════════╪═════════╪═════════╪═════════╪═════════╡\n",
            "│ <|endoftext|>  │ The     │ A       │ This    │ In      │ It      │\n",
            "├────────────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n",
            "│ Paris          │ (       │ ,       │ Saint   │ :       │ -       │\n",
            "├────────────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n",
            "│ is             │ the     │ a       │ set     │ to      │ home    │\n",
            "├────────────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n",
            "│ the            │ capital │ most    │ world   │ city    │ latest  │\n",
            "├────────────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n",
            "│ capital        │ of      │ city    │ and     │ ,       │ for     │\n",
            "├────────────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n",
            "│ of             │ France  │ the     │ Europe  │ French  │ Paris   │\n",
            "╘════════════════╧═════════╧═════════╧═════════╧═════════╧═════════╛\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder-decoder"
      ],
      "metadata": {
        "id": "ln3jAswNPZmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer"
      ],
      "metadata": {
        "id": "CQRT6ajYPbHW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\", forced_bos_token_id=0)\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
        "\n"
      ],
      "metadata": {
        "id": "N8zElxDAPuWp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.special_tokens_map"
      ],
      "metadata": {
        "id": "hY7p0znBRCgX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a452a00-e5a4-430d-9d5a-398184272082"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bos_token': '<s>',\n",
              " 'eos_token': '</s>',\n",
              " 'unk_token': '<unk>',\n",
              " 'sep_token': '</s>',\n",
              " 'pad_token': '<pad>',\n",
              " 'cls_token': '<s>',\n",
              " 'mask_token': '<mask>'}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"A mysterious <mask> is located in Britany.\"\n",
        "tokenized_text = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "print(tokenizer.decode(tokenized_text[0]))\n"
      ],
      "metadata": {
        "id": "5Yqf6KjDQSxH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d80ce4bf-699c-4f5c-9ff9-9179e4cda8f8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>A mysterious<mask> is located in Britany.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval();"
      ],
      "metadata": {
        "id": "lGizBex11cQq"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_ids = model.generate(tokenized_text, do_sample=True, max_new_tokens=20)\n",
        "print(tokenizer.decode(generated_ids[0]))"
      ],
      "metadata": {
        "id": "n7hmGEWHQuyg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d1bedda-1867-4898-a0a0-d0cdb93b0a4d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "</s><s>A mysterious lake in France is located in Britany.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "### Greedy decoding"
      ],
      "metadata": {
        "id": "8zIfbjQ21jg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ],
      "metadata": {
        "id": "ww4VZ40u2A-e"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt = GPT2LMHeadModel.from_pretrained(\"gpt2-large\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
        "gpt.eval();"
      ],
      "metadata": {
        "id": "AgPi1oPzPzZk"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Paris is the capital of\"\n",
        "tokenized_text = tokenizer.encode(text)\n",
        "input_tensor = torch.tensor([tokenizer.bos_token_id] + tokenized_text)[None, :]"
      ],
      "metadata": {
        "id": "AxSF-Z3vP-EJ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_tokens = []\n",
        "gpt.eval()\n",
        "input_sentence = text\n",
        "print(\"Input text:\", input_sentence)\n",
        "input_ids = input_tensor\n",
        "print(\"Input tensor:\", input_tensor)\n",
        "with torch.no_grad():\n",
        "    for i in range(10):\n",
        "        logits = gpt(input_ids).logits\n",
        "        next_token = logits[0, -1].argmax()\n",
        "        input_ids = torch.cat((input_ids, torch.tensor([next_token])[None, :]), dim=-1)\n",
        "        print(\"Current text:\", tokenizer.decode(input_ids[0]))"
      ],
      "metadata": {
        "id": "uBWlAXxaJjXm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac5f80bc-9a42-4841-a618-7e7e5500b831"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input text: Paris is the capital of\n",
            "Input tensor: tensor([[50256, 40313,   318,   262,  3139,   286]])\n",
            "Current text: <|endoftext|>Paris is the capital of France\n",
            "Current text: <|endoftext|>Paris is the capital of France,\n",
            "Current text: <|endoftext|>Paris is the capital of France, and\n",
            "Current text: <|endoftext|>Paris is the capital of France, and the\n",
            "Current text: <|endoftext|>Paris is the capital of France, and the capital\n",
            "Current text: <|endoftext|>Paris is the capital of France, and the capital of\n",
            "Current text: <|endoftext|>Paris is the capital of France, and the capital of the\n",
            "Current text: <|endoftext|>Paris is the capital of France, and the capital of the European\n",
            "Current text: <|endoftext|>Paris is the capital of France, and the capital of the European Union\n",
            "Current text: <|endoftext|>Paris is the capital of France, and the capital of the European Union.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generation_output = gpt.generate(input_ids=input_tensor, do_sample=False, num_beams=1, max_new_tokens=10)\n",
        "tokenizer.decode(generation_output[0])\n"
      ],
      "metadata": {
        "id": "M9CS9HSQ1lJX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d7ae00dc-3f4c-4bb7-b2ca-5ad7d46718c8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|endoftext|>Paris is the capital of France, and the capital of the European Union.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e29cVHt0oFMJ"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}