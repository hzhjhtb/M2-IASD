{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"tep8WojoxBJs","executionInfo":{"status":"ok","timestamp":1699958537468,"user_tz":-60,"elapsed":20725,"user":{"displayName":"Florian Le Bronnec","userId":"09140460650638007404"}}},"outputs":[],"source":["%%capture\n","!pip install datasets transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"FfyQEptMxBJt","executionInfo":{"status":"ok","timestamp":1699958543143,"user_tz":-60,"elapsed":5680,"user":{"displayName":"Florian Le Bronnec","userId":"09140460650638007404"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from tqdm.notebook import tqdm\n","from datasets import Dataset\n","from torch.utils.data import DataLoader\n","import numpy as np\n"]},{"cell_type":"markdown","metadata":{"id":"Bu7fwiOsXM6N"},"source":["# Transformer from scratch\n","\n","In this notebook we are going to code a GPT model from scratch.\n","\n","We will do that in a modular way, and increase the difficulty step by step.\n","\n","## 1. Basic transformer block\n","\n","![](https://drive.google.com/uc?export=view&id=1dlkXQtGtZwoHribpTeQ0MeoNdwoiYJOo)\n","\n","In this first part, we are going to build the grey block. We are going to write this step by step."]},{"cell_type":"markdown","metadata":{"id":"Zn1CP6Y_pFLp"},"source":["ðŸš§ **TODO** ðŸš§\n","\n","Which submodules do we need to write? Propose an architecture for each of them, and keep it as simple as possible.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"13bkeC7UpDpx","executionInfo":{"status":"ok","timestamp":1699958543143,"user_tz":-60,"elapsed":10,"user":{"displayName":"Florian Le Bronnec","userId":"09140460650638007404"}}},"outputs":[],"source":["# TODO"]},{"cell_type":"markdown","metadata":{"id":"GPS16FVRqXi1"},"source":["ðŸš§ **TODO** ðŸš§\n","\n","Embeds this in a `BasicTransformer` class. It should be parametrized by:\n","- `d_embed` the dimension of the input vectors."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"HbUmFESrqXAn","executionInfo":{"status":"ok","timestamp":1699958543144,"user_tz":-60,"elapsed":9,"user":{"displayName":"Florian Le Bronnec","userId":"09140460650638007404"}}},"outputs":[],"source":["# TODO"]},{"cell_type":"markdown","metadata":{"id":"XvejlrPTxBJx"},"source":["ðŸ”´ **TEST**"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"-GFfHab8W6z0","colab":{"base_uri":"https://localhost:8080/","height":248},"executionInfo":{"status":"error","timestamp":1699958543524,"user_tz":-60,"elapsed":388,"user":{"displayName":"Florian Le Bronnec","userId":"09140460650638007404"}},"outputId":"16f70aa8-cf68-4493-9cb7-03d4341ec3a2"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-a30d0d541f94>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBasicTransformerLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'BasicTransformerLayer' is not defined"]}],"source":["d_embed = 8\n","batch_size = 4\n","seq_length = 6\n","for seq_length in [1, 2, 4, 6, 10]:\n","    test_input = torch.randn(batch_size, seq_length, d_embed)\n","    transformer = BasicTransformerLayer(d_embed)\n","    out = transformer(test_input)\n","    assert out.shape == test_input.shape\n"]},{"cell_type":"markdown","metadata":{"id":"LhJ8m1etvhEQ"},"source":["## 2. Masking\n","\n","In practice we all know that within a batch we have some padding tokens."]},{"cell_type":"markdown","metadata":{"id":"UsM9x4quwCXP"},"source":["ðŸš§ **TODO** ðŸš§\n","\n","Update the code above such that it takes as argument an `attention_mask` for padding, and update the code to avoid performing attention on the padding tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lLGUEeKgwA7f","executionInfo":{"status":"aborted","timestamp":1699958543524,"user_tz":-60,"elapsed":14,"user":{"displayName":"Florian Le Bronnec","userId":"09140460650638007404"}}},"outputs":[],"source":["# TODO\n","INF = 1e10"]},{"cell_type":"markdown","metadata":{"id":"Oee63AWYxBJ0"},"source":["ðŸ”´ **TEST**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"niYyi0uXDpTL","executionInfo":{"status":"aborted","timestamp":1699958543525,"user_tz":-60,"elapsed":14,"user":{"displayName":"Florian Le Bronnec","userId":"09140460650638007404"}}},"outputs":[],"source":["def make_random_attention_mask(batch_size, seq_length):\n","    attention_mask = torch.ones(batch_size, seq_length)\n","    max_ind = torch.randint(0, seq_length, (batch_size, 1))\n","    indices = torch.arange(seq_length)\n","    attention_mask[indices >= max_ind] = 0\n","    return attention_mask\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fcORkpSD4Hjw","executionInfo":{"status":"aborted","timestamp":1699958543525,"user_tz":-60,"elapsed":12,"user":{"displayName":"Florian Le Bronnec","userId":"09140460650638007404"}}},"outputs":[],"source":["d_embed = 8\n","batch_size = 4\n","seq_length = 6\n","for seq_length in [2, 4, 6, 10]:\n","    test_input = torch.randn(batch_size, seq_length, d_embed)\n","    transformer = MaskedTransformerLayer(d_embed)\n","    attention_mask = make_random_attention_mask(batch_size, seq_length)\n","    out = transformer(test_input, attention_mask)\n","    assert out.shape == test_input.shape\n"]},{"cell_type":"markdown","metadata":{"id":"2codW7he7-2_"},"source":["ðŸš§ **TODO** ðŸš§\n","\n","Update the code to now account for the causal masking."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oGUj-7T-8ODt","executionInfo":{"status":"aborted","timestamp":1699958543525,"user_tz":-60,"elapsed":12,"user":{"displayName":"Florian Le Bronnec","userId":"09140460650638007404"}}},"outputs":[],"source":["# TODO"]},{"cell_type":"markdown","metadata":{"id":"AxL7lQK4xBJ1"},"source":["ðŸ”´ **TEST**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UlyDxNa58ySI","executionInfo":{"status":"aborted","timestamp":1699958543526,"user_tz":-60,"elapsed":12,"user":{"displayName":"Florian Le Bronnec","userId":"09140460650638007404"}}},"outputs":[],"source":["d_embed = 8\n","batch_size = 4\n","seq_length = 6\n","for seq_length in [2, 4, 6, 10]:\n","    attention_mask = make_random_attention_mask(batch_size, seq_length)\n","    test_input = torch.randn(batch_size, seq_length, d_embed)\n","    transformer = CausalTransformerLayer(d_embed)\n","    out = transformer(test_input, attention_mask)\n","    assert out.shape == test_input.shape\n"]},{"cell_type":"markdown","metadata":{"id":"NZiFK2qWA2CD"},"source":["## 3. Attention heads\n","\n","We are going to update the attention to use multi-head self-attention."]},{"cell_type":"markdown","metadata":{"id":"NTUeX2s9BIS6"},"source":["ðŸš§ **Question** ðŸš§\n","\n","What is multi-heads attention? Explain why it can be useful.\n","\n","**Answer**\n","\n","TODO"]},{"cell_type":"markdown","metadata":{"id":"qmKmwIfCBTYb"},"source":["ðŸš§ **TODO** ðŸš§\n","\n","Update your attention code to use multi-head attention.\n","It should now be parametrised by an additional `num_heads` parameter."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oTl7_Ws5Azyz","executionInfo":{"status":"aborted","timestamp":1699958543526,"user_tz":-60,"elapsed":12,"user":{"displayName":"Florian Le Bronnec","userId":"09140460650638007404"}}},"outputs":[],"source":["# TODO\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ob3rW32BD_EX","executionInfo":{"status":"aborted","timestamp":1699958543526,"user_tz":-60,"elapsed":12,"user":{"displayName":"Florian Le Bronnec","userId":"09140460650638007404"}}},"outputs":[],"source":["d_embed = 8\n","batch_size = 4\n","seq_length = 6\n","for seq_length in [6, 10]:\n","    for num_heads in [1, 2, 4]:\n","        test_input = torch.randn(batch_size, seq_length, d_embed)\n","        transformer = MultiHeadTransformerLayer(d_embed, num_heads)\n","        attention_mask = make_random_attention_mask(batch_size, seq_length)\n","        out = transformer(test_input, attention_mask)\n","        assert out.shape == test_input.shape\n"]},{"cell_type":"markdown","metadata":{"id":"IBtxQ_2h2iZX"},"source":["## 4. Full model\n","![](https://drive.google.com/uc?export=view&id=1dlkXQtGtZwoHribpTeQ0MeoNdwoiYJOo)\n"]},{"cell_type":"markdown","metadata":{"id":"unrOw2ID2pfg"},"source":["ðŸš§ **TODO** ðŸš§\n","\n","Are we good for the internal transformer layer?\n","If something is missing, implement it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NmaHilV92Yws","executionInfo":{"status":"aborted","timestamp":1699958543526,"user_tz":-60,"elapsed":12,"user":{"displayName":"Florian Le Bronnec","userId":"09140460650638007404"}}},"outputs":[],"source":["# TODO\n"]},{"cell_type":"markdown","metadata":{"id":"hHGfRs_83ywj"},"source":["ðŸš§ **Question** ðŸš§\n","\n","We are going to embed this `TransformerLayer` in a complete models.\n","\n","List all the necessary new parameters we need to build such a model.\n","\n","**Answer**\n","\n","TODO\n"]},{"cell_type":"markdown","metadata":{"id":"uTc72z-L26Ty"},"source":["ðŸš§ **TODO** ðŸš§\n","\n","Embed the previously built `TransformerLayer` in a whole `TransformerForCausalLM`.\n","\n","It should be parametrized by:\n","- `d_embeds` the embedding dimension,\n","- `num_head` the number of attention heads,\n","- `n_layers` the number of layers,\n","- the new parameters you listed in the previous question."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ERX1zMZOxBJ4","executionInfo":{"status":"aborted","timestamp":1699958543527,"user_tz":-60,"elapsed":13,"user":{"displayName":"Florian Le Bronnec","userId":"09140460650638007404"}}},"outputs":[],"source":["# TODO\n"]},{"cell_type":"markdown","metadata":{"id":"C9XB8CO-u8zf"},"source":["## 5. Test it on a Random Markov Process!\n","\n","Let's validate the model, on synthetic data."]},{"cell_type":"markdown","metadata":{"id":"cSE5jTQuxBJ4"},"source":["ðŸš§ **Question** ðŸš§\n","\n","Recall what is a Markov Process of order $k$.\n","\n","Why is it a good debugging experience?\n","\n","**Answer**\n","\n","TODO\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16gtlEBkFwTK","executionInfo":{"status":"aborted","timestamp":1699958543527,"user_tz":-60,"elapsed":12,"user":{"displayName":"Florian Le Bronnec","userId":"09140460650638007404"}}},"outputs":[],"source":["def softmax(x, temperature=1.0):\n","    exp_x = np.exp(x / temperature - np.max(x, axis=-1, keepdims=True))\n","    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n","\n","\n","def generate_markov_process(states, transition_matrix, num_steps, order):\n","    if order < 1:\n","        raise ValueError(\"Order should be greater than or equal to 1.\")\n","\n","    current_state = np.random.choice(\n","        states, size=order\n","    )  # Start from a random initial state\n","    state_sequence = np.empty(num_steps, dtype=int)\n","    state_sequence[:order] = current_state\n","\n","    for step in range(order, num_steps):\n","        current_state_index = tuple(current_state)\n","        transition_probs = transition_matrix[current_state_index].flatten()\n","        new_state = np.random.choice(states, p=transition_probs)\n","        state_sequence[step] = new_state\n","        current_state = np.roll(current_state, shift=-1)\n","        current_state[-1] = new_state\n","\n","    return state_sequence\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gon6vR2AMyHL","executionInfo":{"status":"aborted","timestamp":1699958543527,"user_tz":-60,"elapsed":11,"user":{"displayName":"Florian Le Bronnec","userId":"09140460650638007404"}}},"outputs":[],"source":["n_states = 5\n","states = np.arange(n_states)\n","max_length = 32\n","\n","order = 2\n","\n","logits_matrix = np.random.rand(*([n_states] * (order + 1)))\n","temperature = 0.1  # Adjust the temperature as needed\n","transition_matrix = softmax(logits_matrix, temperature=temperature)\n","\n","data = []\n","n = 2000\n","for _ in range(n):\n","    seq_length = np.random.randint(order + 1, max_length)\n","    data.append(\n","        {\n","            \"input_ids\": generate_markov_process(\n","                states, transition_matrix, seq_length, order\n","            ).tolist()\n","        }\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-U-_IT2NRCWa"},"outputs":[],"source":["dataset = Dataset.from_list(data)\n","\n","splitted_dataset = dataset.train_test_split(test_size=0.2)\n","train_dataset = splitted_dataset[\"train\"]\n","valid_dataset = splitted_dataset[\"test\"]"]},{"cell_type":"code","source":[],"metadata":{"id":"PT0xeT7CbCoN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X70CQHeEwLZu"},"source":["ðŸš§ **TODO** ðŸš§\n","\n","Let's build a `DataCollator`.\n","It should take the input given by a batch of the dataset and output:\n","- `input_ids`: the input ids,\n","- `attention_mask`: the attention mask,\n","- `labels`: the labels.\n","\n","> ðŸ’¡ *Hint*: Remember than tokens with ids `-100| are ignored in the CrossEntropyLoss."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6f9KUlfQp_i"},"outputs":[],"source":["# TODO\n"]},{"cell_type":"markdown","metadata":{"id":"87QXYBaNwmVa"},"source":["ðŸš§ **TODO** ðŸš§\n","\n","Train the model and plot the final train and loss curves.\n","\n","We recommend:\n","- `d_embed=128`\n","- `numh_heads=1`\n","- `n_layers=3`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YHfVijKVxBJ6"},"outputs":[],"source":["# TODO\n"]},{"cell_type":"markdown","metadata":{"id":"ZpeWWNHfbujR"},"source":["ðŸš§ **TODO** (Optional) ðŸš§\n","\n","What happens if you use a bidirectional transformer?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wEozHEoab6eB"},"outputs":[],"source":["# TODO\n"]},{"cell_type":"markdown","metadata":{"id":"GZf6dXlBwA5u"},"source":["## 6. Test it on text data!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ZcAhlyaxZv7"},"outputs":[],"source":["from transformers import BartTokenizer\n","from datasets import load_dataset"]},{"cell_type":"markdown","metadata":{"id":"iYkfYy3dxBJ7"},"source":["First load a dataset, then a tokenizer.\n","\n","We choose `BartTokenizer`, but you can choose whatever tokenizer you like."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bfw6ehM1xW6y"},"outputs":[],"source":["dataset = load_dataset(\"scikit-learn/imdb\", split=\"train\")\n","print(dataset)\n","tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"70x_XxO5wrSq"},"outputs":[],"source":["def preprocessing_fn(x, tokenizer):\n","    x[\"input_ids\"] = tokenizer.encode(\n","        x[\"review\"],\n","        add_special_tokens=False,\n","        truncation=True,\n","        max_length=128,\n","        padding=False,\n","        return_attention_mask=False,\n","    )\n","    return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IGmFzJ1txq81"},"outputs":[],"source":["n_samples = 5000  # the number of training example\n","\n","# We first shuffle the data !\n","dataset = dataset.shuffle()\n","\n","splitted_dataset = dataset.select(range(n_samples))\n","\n","# Tokenize the dataset\n","splitted_dataset = splitted_dataset.map(\n","    preprocessing_fn, fn_kwargs={\"tokenizer\": tokenizer}\n",")\n","\n","\n","# Remove useless columns\n","splitted_dataset = splitted_dataset.select_columns([\"input_ids\"])\n","\n","# Split the train and validation\n","splitted_dataset = splitted_dataset.train_test_split(test_size=0.2)\n","\n","train_set = splitted_dataset[\"train\"]\n","valid_set = splitted_dataset[\"test\"]"]},{"cell_type":"markdown","metadata":{"id":"v9wXXLd7xBJ7"},"source":["ðŸš§ **TODO** ðŸš§\n","\n","Update the `DataCollator` such that it is now compatible with texts data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9IVUuJguxzvx"},"outputs":[],"source":["# TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EOP9BwKEyMNT"},"outputs":[],"source":["batch_size = 64\n","\n","train_dataloader = DataLoader(\n","    train_set, batch_size=batch_size, collate_fn=data_collator\n",")\n","valid_dataloader = DataLoader(\n","    valid_set, batch_size=batch_size, collate_fn=data_collator\n",")\n","n_valid = len(valid_set)\n","n_train = len(train_set)"]},{"cell_type":"markdown","metadata":{"id":"aNOsYUK0xBJ8"},"source":["ðŸš§ **TODO** ðŸš§\n","\n","Build a real transformer model. You can call it `MyGPT`.\n","\n","We recommend:\n","\n","- `d_embed=256`\n","- `num_heads=4`\n","- `n_layers=4`\n","- `max_length=4`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GUJKScTAyTYc"},"outputs":[],"source":["# TODO"]},{"cell_type":"markdown","metadata":{"id":"u-hNJnnCdIT9"},"source":["ðŸš§ **Question** ðŸš§\n","\n","What is the size of the model?\n","\n","What are the biggest layers in terms of number of parameters?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ucDw7W9GxBJ9"},"outputs":[],"source":["def count_parameters(model):\n","    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    return f\"{total_params:,}\""]},{"cell_type":"markdown","metadata":{"id":"Tc0gNtpuxBJ9"},"source":["ðŸš§ **TODO** ðŸš§\n","\n","Train your GPT! Make sure to use CUDA, and track the training and validation loss."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LXAHau2oy_3K"},"outputs":[],"source":["# TODO"]},{"cell_type":"markdown","metadata":{"id":"RmSTJr4Ieobm"},"source":["ðŸš§ **Question** ðŸš§\n","\n","Plot the training and validation curves. What can you conclude from those curves?\n","\n","**Answer**\n","\n","TODO"]},{"cell_type":"markdown","metadata":{"id":"BfmiI_qXcpVV"},"source":["ðŸš§ **TODO** ðŸš§\n","\n","Implement an greedy decoding algorithm. And use it on your model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oci1wSFG1Jdv"},"outputs":[],"source":["# TODO"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}