{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LiDAR-LLM\n",
    "\n",
    "Student Name: **Zhe HUANG** (from Master IASD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VoxelNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Connected Network\n",
    "class FCN(nn.Module):\n",
    "\n",
    "    def __init__(self, cin, cout):\n",
    "        super(FCN, self).__init__()\n",
    "        self.cout = cout\n",
    "        self.linear = nn.Linear(cin, cout)\n",
    "        self.bn = nn.BatchNorm1d(cout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # KK is the stacked k across batch\n",
    "        kk, t, _ = x.shape\n",
    "        x = self.linear(x.view(kk * t, -1))\n",
    "        x = F.relu(self.bn(x))\n",
    "        return x.view(kk, t, -1)\n",
    "\n",
    "# Voxel Feature Encoding layer\n",
    "class VFE(nn.Module):\n",
    "\n",
    "    def __init__(self, cin, cout):\n",
    "        super(VFE, self).__init__()\n",
    "        assert cout % 2 == 0\n",
    "        self.units = cout // 2\n",
    "        self.fcn = FCN(cin, self.units)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # point-wise feature\n",
    "        pwf = self.fcn(x)\n",
    "        # locally aggregated feature\n",
    "        laf = torch.max(pwf, 1)[0].unsqueeze(1).repeat(1, cfg.T, 1)\n",
    "        # point-wise concat feature\n",
    "        pwcf = torch.cat((pwf, laf), dim=2)\n",
    "        # apply mask\n",
    "        mask = mask.unsqueeze(2).repeat(1, 1, self.units * 2)\n",
    "        pwcf = pwcf * mask.float()\n",
    "\n",
    "        return pwcf\n",
    "\n",
    "# Stacked Voxel Feature Encoding\n",
    "class SVFE(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SVFE, self).__init__()\n",
    "        self.vfe_1 = VFE(7, 32)\n",
    "        self.vfe_2 = VFE(32, 128)\n",
    "        self.fcn = FCN(128, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = torch.ne(torch.max(x, 2)[0], 0)\n",
    "        x = self.vfe_1(x, mask)\n",
    "        x = self.vfe_2(x, mask)\n",
    "        x = self.fcn(x)\n",
    "        # element-wise max pooling\n",
    "        x = torch.max(x, 1)[0]\n",
    "        return x\n",
    "\n",
    "\n",
    "class VoxelNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VoxelNet, self).__init__()\n",
    "        self.svfe = SVFE()\n",
    "\n",
    "    def forward(self, point_clouds):\n",
    "        # Feature learning network\n",
    "        voxel_features = self.svfe(point_clouds)\n",
    "        return voxel_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiDAR3DFeatureExtractor:\n",
    "    def __init__(self, model_path, point_cloud_range=(-54.0, 54.0, -5.0, 54.0, 54.0, 3.0), bev_grid_size=(0.6, 0.6)):\n",
    "        self.model = VoxelNet()\n",
    "        self.model.load_state_dict(torch.load(model_path))  # Load the pretrained model weights\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "        \n",
    "        self.point_cloud_range = point_cloud_range\n",
    "        self.bev_grid_size = bev_grid_size\n",
    "\n",
    "    def extract_features(self, point_cloud):\n",
    "        \"\"\"\n",
    "        Extracts 3D voxel features from a LiDAR point cloud.\n",
    "        \n",
    "        :param point_cloud: A numpy array of shape (n, 3) representing the LiDAR point cloud.\n",
    "        :return: 3D voxel feature tensor.\n",
    "        \"\"\"\n",
    "        # Normalize point cloud within specified range\n",
    "        point_cloud_normalized = self.normalize_point_cloud(point_cloud)\n",
    "        \n",
    "        point_cloud_tensor = torch.tensor(point_cloud_normalized, dtype=torch.float32)\n",
    "        voxel_feature = self.model(point_cloud_tensor.unsqueeze(0))  # Adding a batch dimension\n",
    "        \n",
    "        return voxel_feature\n",
    "\n",
    "    def normalize_point_cloud(self, point_cloud):\n",
    "        \"\"\"\n",
    "        Normalize the LiDAR point cloud within the specified range.\n",
    "        \n",
    "        :param point_cloud: A numpy array of shape (n, 3) representing the LiDAR point cloud.\n",
    "        :return: Normalized point cloud.\n",
    "        \"\"\"\n",
    "        # Assuming point_cloud_range is in format (min_x, max_x, min_y, max_y, min_z, max_z)\n",
    "        min_x, max_x, min_y, max_y, min_z, max_z = self.point_cloud_range\n",
    "        \n",
    "        # Normalize x, y, z coordinates\n",
    "        point_cloud[:, 0] = (point_cloud[:, 0] - min_x) / (max_x - min_x)\n",
    "        point_cloud[:, 1] = (point_cloud[:, 1] - min_y) / (max_y - min_y)\n",
    "        point_cloud[:, 2] = (point_cloud[:, 2] - min_z) / (max_z - min_z)\n",
    "        \n",
    "        return point_cloud\n",
    "\n",
    "    def flatten_feature_to_bev(self, voxel_feature):\n",
    "        \"\"\"\n",
    "        Flattens the 3D voxel feature along the z-axis to generate a BEV feature.\n",
    "        \n",
    "        :param voxel_feature: A tensor representing the 3D voxel feature.\n",
    "        :return: BEV feature tensor.\n",
    "        \"\"\"\n",
    "        # Assuming bev_grid_size is in format (grid_size_x, grid_size_y)\n",
    "        grid_size_x, grid_size_y = self.bev_grid_size\n",
    "        \n",
    "        # Perform max pooling along the z-axis\n",
    "        bev_feature = voxel_feature.max(dim=-1)[0]\n",
    "        \n",
    "        # Reshape the feature tensor to match the BEV grid size using nearest neighbor interpolation\n",
    "        bev_feature = F.interpolate(bev_feature.unsqueeze(0).unsqueeze(0), size=(grid_size_x, grid_size_y), mode='nearest')\n",
    "        \n",
    "        return bev_feature.squeeze(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View-Aware Transfomer(VAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAT(nn.Module):\n",
    "    def __init__(self, num_queries=576, input_dim=768, output_dim=768, num_view_positions=6):\n",
    "        super(VAT, self).__init__()\n",
    "        self.num_queries = num_queries\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_view_positions = num_view_positions\n",
    "        \n",
    "        # Learnable query embeddings\n",
    "        self.query_embeddings = nn.Parameter(torch.randn(num_queries, input_dim))\n",
    "        \n",
    "        # View position embeddings\n",
    "        self.view_position_embeddings = nn.Parameter(torch.zeros(input_dim, num_view_positions))\n",
    "        \n",
    "        # Cross-attention mechanism\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=output_dim, num_heads=8)\n",
    "        \n",
    "        # MLP for processing visual vectors\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(output_dim, output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(output_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, visual_feature):\n",
    "        # Repeat query embeddings to match the batch size of visual features\n",
    "        queries = self.query_embeddings.unsqueeze(0).repeat(visual_feature.size(0), 1, 1)\n",
    "        \n",
    "        # Add view position embeddings to the BEV feature\n",
    "        visual_feature_with_view_position = visual_feature + self.view_position_embeddings.unsqueeze(0)\n",
    "        \n",
    "        # Cross-attention mechanism\n",
    "        attn_output, _ = self.cross_attention(queries, visual_feature_with_view_position, visual_feature_with_view_position)\n",
    "        \n",
    "        # Apply MLP to process visual vectors\n",
    "        processed_output = self.mlp(attn_output.transpose(0, 1))\n",
    "        \n",
    "        return processed_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pretrained model 'dfurman/llama-7b' and the tokenizer\n",
    "model_name = \"dfurman/llama-7b\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "llama_model = transformers.LlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "class LiDAR_LLMA(nn.Module):\n",
    "    def __init__(self, llama_model, mlp_hidden_dim):\n",
    "        super(LiDAR_LLMA, self).__init__()\n",
    "        self.llama_model = llama_model\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(mlp_hidden_dim, mlp_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_dim, mlp_hidden_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, text_input_ids, text_attention_mask):\n",
    "        # Run LLAMA model\n",
    "        output = self.llama_model(\n",
    "            input_ids=text_input_ids,\n",
    "            attention_mask=text_attention_mask\n",
    "        )\n",
    "        # Process output through MLP\n",
    "        visual_vectors = output.logits  # Assuming logits contain visual vectors\n",
    "        processed_vectors = self.mlp(visual_vectors)\n",
    "        return processed_vectors\n",
    "\n",
    "\n",
    "# Instantiate LiDAR-LLMA model\n",
    "lidar_llma_model = LiDAR_LLMA(llama_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IASD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
